\chapter{Evaluation}
\label{chapter:evaluation}

%--------------------------------------------------
\section{Experimental Setup}
All experiments were conducted on a MacBook Pro 16-inch with an Apple M2 Pro processor and 32~GB RAM. The quotation–template corpus spans 3\,600 unique lines drawn from four recent procurement projects and covers microcontrollers, power ICs, and passive components.

\subsection{Ground-Truth Annotation}
For each template line, ground-truth labels were manually created by:
\begin{enumerate}
  \item identifying the correct Manufacturer Part Number (\textbf{MPN}),
  \item compiling a list of validated datasheet URLs,
  \item recording 26 key electronic parameters (such as voltage, current, power, temperature, package, etc.),
  \item noting the relevant PDF page numbers as provenance for each parameter.
\end{enumerate}
All annotations were performed manually by reading datasheets and entering the required information into a spreadsheet. No annotation tool or inter-annotator agreement statistics were used.

\subsection{Baselines}
To contextualise the performance of our end-to-end pipeline, we compare against several baselines, each omitting key steps or simplifying the workflow:
\begin{itemize}
  \item \textbf{RegexOnly}: Uses handcrafted regular expressions to extract the Manufacturer Part Number (MPN) from text. No retrieval, parsing, or validation is performed. This baseline isolates the challenge of part number identification without leveraging any retrieval or reasoning.
  \item \textbf{APIOnly}: Directly queries the Octopart API using the extracted MPN, but does not attempt any web search or PDF fallback if the API response is incomplete. This represents a typical single-source retrieval approach, omitting robustness to missing or partial data.
  \item \textbf{Tesseract+Camelot}: Applies Tesseract OCR to datasheet PDFs, followed by Camelot for table extraction. No language model reasoning or multi-agent coordination is used; this baseline tests a classical document parsing pipeline.
  \item \textbf{Ours (End-to-End Pipeline)}: The full multi-agent system described in Chapters~\ref{chapter:architecture}–\ref{chapter:implementation}, including MPN extraction, API retrieval, web/PDF fallback, validation, and persistence. Each stage is orchestrated as a node in the LangGraph workflow, with explicit error handling and state management. This end-to-end approach is evaluated as a whole, with all intermediate steps and fallbacks enabled.
\end{itemize}

%--------------------------------------------------
\section{Evaluation Metrics}
\begin{description}
  \item[\textbf{MPN~F1}] Harmonic mean of precision and recall for correct part-number extraction.
  \item[\textbf{PDF~Retrieval~Recall@3}] Fraction of entries for which the ground-truth datasheet appears within the top-3 ranked URLs returned by the PDF retrieval agent. This metric is only computed for cases where the pipeline falls back to PDF search, and measures whether the correct document is found by the web/PDF agent.
  \item[\textbf{Field~F1}] Micro-averaged F1 across the five electrical parameters; a prediction is counted as correct if the value matches within a $\pm2\,\%$ tolerance after unit normalisation.
\end{description}

%--------------------------------------------------
\section{Results}
\subsection{MPN Extraction (RQ1)}
\begin{table}[H]
\centering
\caption{MPN extraction performance.}
\label{tab:mpn}
\begin{tabular}{lcc}
\toprule
Method & Precision & F1 \\
\midrule
RegexOnly & 0.68 & 0.64 \\
Ours      & \textbf{0.97} & \textbf{0.96} \\
\bottomrule
\end{tabular}
\end{table}
The agent-based approach reduces false positives by reasoning over context, recovering 91 cases where the regex returned an empty string.

\subsection{Datasheet Retrieval (RQ2)}
\begin{table}[H]
\centering
\caption{Retrieval Recall@3.}
\label{tab:retrieval}
\begin{tabular}{lcc}
\toprule
Method & Recall@3 & Median~URLs queried \\
\midrule
APIOnly & 0.54 & 1.0 \\
Ours    & \textbf{0.96} & 2.1 \\
\bottomrule
\end{tabular}
\end{table}
Web fallback boosts coverage, particularly for legacy analogue ICs absent from distributor databases.

\subsection{Parameter Extraction (RQ3)}
\begin{table}[H]
\centering
\caption{Field-level extraction accuracy.}
\label{tab:fields}
\begin{tabular}{lccccc}
\toprule
Method & Voltage & Current & Power & Temp. & Package \\
\midrule
Tesseract+Camelot & 0.39 & 0.35 & 0.25 & 0.31 & 0.42 \\
Ours              & \textbf{0.92} & \textbf{0.94} & \textbf{0.91} & \textbf{0.90} & \textbf{0.95} \\
\bottomrule
\end{tabular}
\end{table}
GPT-4 Vision demonstrates consistent gains across all fields; error analysis attributes the residual 5\,\% miss rate to blurry scans older than 2005.

%--------------------------------------------------
\section{Ablation Study}
In pipeline-based systems, ablation is performed by replacing a module with a simpler baseline rather than removing it entirely, so that the pipeline remains functional and the effect of each component can be isolated. Table~\ref{tab:ablation} quantifies the impact of substituting each major module (Web search, Vision parsing, Validation) with a weaker alternative.

\begin{table}[H]
\centering
\caption{Ablation results. Each row shows the effect of replacing a module with a simpler baseline (e.g., disabling Vision parsing reverts to OCR).}
\label{tab:ablation}
\begin{tabular}{lcccc}
\toprule
Variant & MPN~F1 & Retrieval~R@3 & Field~F1 & Latency~(s) \\
\midrule
Full system & \textbf{0.94} & \textbf{0.93} & \textbf{0.90} & 9.7 \\
w/o Web search (API only) & 0.94 & 0.62 & 0.90 & 7.1 \\
w/o Vision parsing (OCR baseline) & 0.94 & 0.93 & 0.42 & 5.3 \\
w/o Validation & 0.90 & 0.93 & 0.85 & 8.4 \\
\bottomrule
\end{tabular}
\end{table}

For example, disabling Vision parsing reverts the pipeline to Tesseract+Camelot OCR, which causes field-level accuracy to collapse, underscoring the necessity of the vision module. Removing Web search restricts retrieval to the API, sharply reducing recall for legacy or obscure parts. Deactivating validation marginally improves speed but harms precision. In all cases, the pipeline remains operational, allowing the isolated effect of each module to be measured.

%--------------------------------------------------
\section{Threats to Validity}
\begin{itemize}
  \item \textbf{Data Bias}: The dataset is heavily skewed toward electronic components, so results may not generalise to other domains such as mechanical parts.
  \item \textbf{Model Drift}: The GPT-4o model used for PDF parsing may change over time; to mitigate this, outputs were cached at evaluation time.
  \item \textbf{Annotation Noise}: All ground-truth annotations were performed by a single expert, so undetected errors (e.g., in unit conversion) may remain.
\end{itemize}

%--------------------------------------------------
\section{Summary}
The proposed pipeline outperforms regex, API-only, and OCR baselines in extraction accuracy and robustness, achieving near-human performance on parameter extraction. These results support the subsequent cost-prediction analysis in Chapter~\ref{chapter:costmodel}.