\chapter{System Architecture}
\label{chapter:architecture}

% Usage Instructions
% Insert this code in place of your current \section{System Architecture} placeholder.
% 
% Add \usepackage{tikz} and \usepackage{enumitem} if not already present.
% 
% Replace or augment the heuristic equation, table, or diagram details to match your final implementation.



This chapter describes the overall design of the proposed multi-agent pipeline, the responsibilities of each agent, their communication strategy, and the supporting infrastructure for storage, logging, and fault-recovery.

\section{High-Level Overview}

Figure~\ref{fig:architecture} illustrates the data flow from a raw standardised quotation analysis template line to a validated specification record and a downstream cost prediction.  
Each rectangular node represents an autonomous agent implemented as a function-calling wrapper around a large language model (LLM) or vision-language model (VLM).  
Solid arrows indicate the primary execution path, while dashed arrows represent feedback loops used for retries and validation.


\begin{figure}
  \centering
  \begin{tikzpicture}[
    node distance=1.5cm and 2.5cm,
    every node/.style={font=\small},
    start/.style={rectangle, rounded corners=3pt, draw, minimum width=4cm, minimum height=1cm, align=center},
    planner/.style={rectangle, draw, minimum width=6cm, minimum height=1.2cm, align=center, anchor=north},
    agent/.style={rectangle, draw, minimum width=5cm, minimum height=1.1cm, align=center, anchor=north},
    fusion/.style={rectangle, draw, minimum width=6cm, minimum height=1.2cm, align=center},
    validation/.style={rectangle, draw, minimum width=5cm, minimum height=1.1cm, align=center},
    storage/.style={cylinder, draw, minimum height=1.1cm, minimum width=2.5cm, align=center},
    cost/.style={rectangle, draw, minimum width=5cm, minimum height=1.1cm, align=center}
  ]
    % Top node
    \node[start] (start) {Start: MPN Input};

    % Planner
    \node[planner, below=of start] (planner) {MPN Agent -- MPN Extraction};

    % Two agents
    \node[agent, below left=1.5cm and 3.5cm of planner.south] (api) {Retrieval Agent -- API Access};
    \node[agent, below=1.5cm of planner] (pdf) {Retrieval Agent -- PDF Parsing};

    % Field Fusion
    \node[fusion, below=2.2cm of pdf] (fusion) {Field Fusion -- Schema Resolution};

    % Validation Module
    \node[validation, below=1.5cm of fusion] (validation) {Validation Module};

    % Storage
    \node[storage, below=1.5cm of validation] (storage) {Spec DB};

    % Cost Model
    \node[cost, below=1.5cm of storage] (cost) {Cost Model};

    % Arrows
    \draw[->] (start) -- (planner);
    \draw[->] (planner) -- (api);
    \draw[->] (planner) -- (pdf);
    \draw[->] (api) -- (fusion);
    \draw[->] (pdf) -- (fusion);
    \draw[->] (fusion) -- (validation);
    \draw[->] (validation) -- node[right]{valid} (storage);
    \draw[->] (storage) -- (cost);

    % Feedback loop for invalid case
    \draw[->, dashed]
      (validation.east)
      .. controls +(3,0) and +(3,0) .. (planner.north east)
      node[midway, right, align=center] {invalid};

  \end{tikzpicture}
  \caption{Multi-agent pipeline: vertical, layered architecture with validation and feedback.}
  \label{fig:architecture}
\end{figure}



\section{Agent Responsibilities}
\paragraph{MPN Agent}  
The MPN Agent receives a raw standardised quotation analysis template line, extracts the most likely Manufacturer Part Number (MPN) using a few-shot GPT-4 prompt, and forwards the extracted MPN to the Retrieval Agent. The Retrieval Agent then attempts to find the technical parameters of the component based on the provided MPN.


\paragraph{Retrieval Agent (API)}  
Queries the Octopart REST API with the extracted MPN.  
If the API returns \verb|datasheets| with a direct PDF link and the file size is within 50 kB–20 MB, the link is forwarded to the Parsing Agent.

\paragraph{Retrieval Agent (PDF)}  
Uses Playwright to perform a Google search of the form  
\texttt{\{MPN\} datasheet filetype:pdf}.  
% Results are scored by the reliability heuristic in Equation~\eqref{eq:heuristic}.  
The most likely PDFs are downloaded for validation.
after the pdfs are downloaded, the pdf agent will use a chain of thought prompt to extract the most likely parameters from the pdf.

\paragraph{Field Fusion}
The field fusion module will take the output from the retrieval agents and compare them to the standardised quotation analysis template.
The field will output a json object with the most likely parameters, the output of the field fusion module is then passed to the validation module.

\paragraph{Validation Module}  
Executes three layers of checks:
\begin{enumerate}
  \item JSON-Schema validation (types + units),
  \item range rules (e.\,g.\ $0 < V_{max} < 1000V$),
\end{enumerate}  
Failures trigger a dashed feedback edge to the Web Retrieval Agent for retry.

\paragraph{Cost Model Example}  
As a sample downstream task, a cost model is implemented by training an random forest regressor on validated specification records and historical prices. This demonstrates how the structured output can be used for economic analysis or price prediction.

\section{Agent Coordination and Memory}
Agents are orchestrated using the LangGraph framework, which enables flexible, graph-based workflows and agent composition.  
\begin{itemize}
  \item \textbf{Function-calling interface}: Each agent exposes a JSON schema; GPT-4 (or other LLM) selects and invokes the appropriate function based on the current task node in the graph.
  \item \textbf{Shared State}: LangGraph maintains a mutable state object that is passed between agents as the workflow progresses. This state contains intermediate results (such as \verb|mpn|, \verb|pdf_path|, validation flags) and is accessible to all agents at each step, enabling coordination without explicit external storage.
  \item \textbf{Retry Logic}: Retry and fallback behavior is implemented within the graph structure—nodes can be configured to handle exceptions, apply exponential backoff (1 s, 2 s, 4 s), and reroute control to fallback agents if the number of attempts exceeds 3.
\end{itemize}

\section{Source Ranking Heuristic}
When ranking candidate PDF URLs for datasheet retrieval, the system evaluates each source using a composite cost score that reflects both technical and content-based criteria. For each candidate URL $u$, the following factors are considered:
\begin{itemize}
    \item \textbf{Domain Trust Level:} Manufacturer domains are preferred (score 0), followed by trusted distributors (score 1), with all other domains scored as 2.
    \item \textbf{Direct PDF Link:} URLs ending with \texttt{.pdf} are favored (score 0), while indirect or non-PDF links are penalized (score 1).
    \item \textbf{Historical Failure Rate:} An exponentially decayed average of previous download failures from the same host, to deprioritize unreliable sources.
    \item \textbf{PDF Size:} Files are checked to ensure they fall within an acceptable size range (e.g., 50 kB–20 MB); extremely small or large files are penalized.
    \item \textbf{Number of Pages:} PDFs with a plausible number of pages for a datasheet (e.g., 1–20) are preferred; outliers are deprioritized.
    \item \textbf{Keyword Presence:} The system scans the PDF (or its metadata) for key terms such as ``specification,'' ``datasheet,'' or the target MPN, boosting the score for matches.
\end{itemize}
The overall cost is computed as a weighted sum of these features:
\begin{equation}
\label{eq:heuristic}
\mathrm{cost}(u) = w_1 \cdot \mathrm{domain}(u) + w_2 \cdot \mathrm{pdfWeight}(u) + w_3 \cdot \mathrm{failRate}(u) + w_4 \cdot \mathrm{sizePenalty}(u) + w_5 \cdot \mathrm{pagePenalty}(u) - w_6 \cdot \mathrm{keywordBonus}(u)
\end{equation}
where $w_1, \ldots, w_6$ are tunable weights reflecting the importance of each criterion. Lower cost scores indicate higher-priority candidates for download and parsing.

\section{Data Storage and Logging}
After downloading each PDF, the file is uploaded to Azure Blob Storage. The Azure Blob link, together with its original source URL, is stored in a PostgreSQL database hosted on Azure. The validated specification fields are stored directly in dedicated columns of the PostgreSQL table (rather than as a JSON blob), along with the \verb|mpn|, provenance information, and the Azure Blob link and origin.

\section{Fault Tolerance}
If both retrieval agents fail, the system stores a \verb|status = "unresolved"| entry for manual triage.  
Parsing errors with low OCR confidence ($<0.7$) raise a warning but still save partially validated fields, ensuring \emph{at-least-once} data capture.

\section{Mapping to Requirements}
Table~\ref{tab:reqmap} summarises how architectural components satisfy the automation requirements defined in Chapter~\ref{sec:problem}.

\begin{table}[ht]
  \centering
  \caption{Requirements coverage by architectural component.}
  \label{tab:reqmap}
  \begin{tabular}{ll}
    \hline
    Requirement & Fulfilled by \\\hline
    R1 Robust MPN extraction & MPN Extraction Agent (LLM + regex) \\
    R2 Adaptive retrieval    & API + Web Retrieval Agents, heuristic ranking \\
    R3 Multi-modal parsing   & VLM Parsing Agent (GPT-4 Vision) \\
    R4 Schema validation     & Validation Module (JSON Schema, range rules) \\
    R5 Traceability          & Provenance storage (page, bbox) \\
    R6 Scalability           & Redis memory, async Playwright, GPU offload \\\hline
  \end{tabular}
\end{table}

Together, these design choices create a fault-tolerant, interpretable, and extensible architecture capable of meeting BMW's data-quality and throughput requirements.
