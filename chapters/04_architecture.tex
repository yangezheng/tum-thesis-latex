\chapter{System Architecture}
\label{chapter:architecture}

This chapter describes the overall design of the proposed multi-agent pipeline, the responsibilities of each agent, their communication strategy, and the supporting infrastructure for storage, logging, and fault-recovery.

\section{High-Level Overview}

Figure~\ref{fig:architecture} illustrates the data flow from a raw Material Pricing Sheet line to a validated specification record and a downstream cost prediction.  
Each rectangular node represents an autonomous agent implemented as a function-calling wrapper around a large language model (LLM) or vision-language model (VLM).  
Solid arrows indicate the primary execution path, while dashed arrows represent feedback loops used for retries and validation.


\begin{figure}
  \centering
  \begin{tikzpicture}[
    node distance=1.5cm and 2.5cm,
    every node/.style={font=\small},
    start/.style={rectangle, rounded corners=3pt, draw, minimum width=4cm, minimum height=1cm, align=center},
    planner/.style={rectangle, draw, minimum width=6cm, minimum height=1.2cm, align=center, anchor=north},
    agent/.style={rectangle, draw, minimum width=5cm, minimum height=1.1cm, align=center, anchor=north},
    fusion/.style={rectangle, draw, minimum width=6cm, minimum height=1.2cm, align=center},
    validation/.style={rectangle, draw, minimum width=5cm, minimum height=1.1cm, align=center},
    storage/.style={cylinder, draw, minimum height=1.1cm, minimum width=2.5cm, align=center},
    cost/.style={rectangle, draw, minimum width=5cm, minimum height=1.1cm, align=center}
  ]
    % Top node
    \node[start] (start) {Start: Component Identifier Input};

    % Planner
    \node[planner, below=of start] (planner) {Coordinator Agent -- Component Identifier Extraction};

    % Two agents
    \node[agent, below left=1.5cm and 3.5cm of planner.south] (api) {Retrieval Agent -- API Access};
    \node[agent, below=1.5cm of planner] (pdf) {Retrieval Agent -- PDF Parsing};

    % Field Fusion
    \node[fusion, below=2.2cm of pdf] (fusion) {Field Fusion -- Schema Resolution};

    % Validation Module
    \node[validation, below=1.5cm of fusion] (validation) {Validation Module};

    % Storage
    \node[storage, below=1.5cm of validation] (storage) {Spec DB};

    % Cost Model
    \node[cost, below=1.5cm of storage] (cost) {Cost Model};

    % Arrows
    \draw[->] (start) -- (planner);
    \draw[->] (planner) -- (api);
    \draw[->] (planner) -- (pdf);
    \draw[->] (api) -- (fusion);
    \draw[->] (pdf) -- (fusion);
    \draw[->] (fusion) -- (validation);
    \draw[->] (validation) -- node[right]{valid} (storage);
    \draw[->] (storage) -- (cost);

    % Feedback loop for invalid case
    \draw[->, dashed]
      (validation.east)
      .. controls +(3,0) and +(3,0) .. (planner.north east)
      node[midway, right, align=center] {invalid};

  \end{tikzpicture}
  \caption{Multi-agent pipeline: vertical, layered architecture with validation and feedback.}
  \label{fig:architecture}
\end{figure}



\section{Agent Responsibilities}

\paragraph{Coordinator Agent}  
The Coordinator Agent is the entry point of the workflow. For every incoming Material Pricing Sheet line, it invokes the \emph{Component~Identifier~Extraction} tool—an LLM function call that returns the most likely component identifier in JSON format. The extracted component identifier is then forwarded to the Retrieval Agents for technical‐parameter lookup. By unconditionally routing all inputs through the same extraction step, the system eliminates heuristic shortcuts and ensures uniform preprocessing.


\paragraph{Retrieval Agent (API)}  
Queries the Octopart REST API with the extracted component identifier to directly obtain structured technical parameters for the component.  
For each supported component category, the agent extracts the predefined set of technical fields from the API response and forwards them for schema resolution and validation.  
No PDF retrieval or parsing is required in this path, making the process more efficient and reliable than PDF-based extraction.

\paragraph{Retrieval Agent (PDF)}  
When the API does not provide sufficient technical parameters, the system invokes the PDF Retrieval Agent. This agent uses Playwright to automate a Google search with the query  
\texttt{\{component identifier\} datasheet filetype:pdf}.  
Candidate PDF URLs are ranked using a reliability heuristic (see Equation~\ref{eq:heuristic}), prioritizing trusted sources and plausible datasheet characteristics. The top-ranked PDFs are downloaded for further analysis.  
After downloading, the agent applies a vision language model (e.g., GPT-4 Vision) with a chain-of-thought prompt to parse the datasheet and extract the most likely technical parameters directly from the PDF content.

\paragraph{Field Fusion}
The field fusion module merges and reconciles outputs from multiple retrieval agents (API and PDF extraction) to produce a single, most likely set of technical parameters in a standardized format. It resolves conflicts and fills gaps by comparing agent outputs against the Material Pricing Sheet, ensuring the best possible data is selected for each field. The result is a JSON object containing the fused parameters, which is then passed to the validation module for further checks.

\paragraph{Validation Module}  
The validation module applies three layers of checks:
\begin{enumerate}
  \item \textbf{JSON-Schema validation:} Ensures that all extracted fields have the correct types and units.
  \item \textbf{Physical range rules:} Verifies that values fall within plausible physical limits (e.g., $0 < V_{\max} < 1000\,\mathrm{V}$).
\end{enumerate}
If any check fails, a dashed feedback edge routes control back to the \emph{Coordinator Agent} for another attempt.

\paragraph{Cost Model Example}  
A major downstream application enabled by the structured data pipeline is automated cost estimation. This is achieved by constructing a cost model that predicts the likely unit price of a component based on its technical attributes. The process can be broken down into the following steps:

\begin{itemize}
  \item \textbf{Downstream Application:}  
    Automated cost estimation using structured data extracted by the multi-agent system.
  
  \item \textbf{Model Construction:}
    \begin{itemize}
      \item Train a random forest regressor.
      \item Use features from validated specification records (e.g., voltage, current, package type, and other technical parameters).
      \item Pair these features with historical price data.
    \end{itemize}
  
  \item \textbf{Training Process:}
    \begin{itemize}
      \item Aggregate high-confidence, schema-compliant records from the database.
      \item Perform feature engineering (such as normalizing voltages and encoding package types).
      \item Train and evaluate the model using historical pricing as the target variable.
    \end{itemize}
  
  \item \textbf{Integration:}
    \begin{itemize}
      \item Integrate the trained cost model into the pipeline.
      \item As soon as a new component's parameters are extracted and validated, feed them into the regressor.
      \item Generate real-time price estimates for new or unseen parts.
    \end{itemize}
  
  \item \textbf{Benefits:}
    \begin{itemize}
      \item Enables automated quotation, procurement, and price benchmarking.
      \item Unlocks downstream machine learning applications for economic analysis, price optimization, and decision support.
    \end{itemize}
\end{itemize}

This structured, list-based approach highlights each stage of the cost model pipeline and emphasizes how the multi-agent system's outputs facilitate advanced ML-driven applications.

\section{Agent Coordination and Memory}
Agent coordination is implemented using the LangGraph framework, which enables flexible, graph-based orchestration of specialized agents. The system exclusively uses GPT-4o as the function-calling LLM at each node, allowing for consistent, high-quality reasoning and function selection throughout the workflow.

\begin{table}[H]
  \centering
  \caption{Key aspects of agent coordination with LangGraph and GPT-4o.}
  \begin{tabular}{p{3.5cm} p{10cm}}
    \hline
    \textbf{Aspect} & \textbf{Description} \\
    \hline
    Function-calling interface & Each agent exposes a JSON schema describing its function signature. GPT-4o selects and invokes the appropriate agent function at each node, using the schema to structure inputs and outputs. \\
    Shared state & LangGraph maintains a mutable state object (e.g., \verb|component_identifier|, \verb|pdf_path|, validation flags, extracted parameters) that is passed between nodes. All agents can read and update this state, enabling seamless context sharing and coordination without external storage. \\
    Retry and fallback logic & The workflow graph encodes retry and fallback behavior: nodes can handle exceptions, apply exponential backoff (e.g., 1\,s, 2\,s, 4\,s), and reroute control to fallback agents or alternative paths if a step fails repeatedly (typically after 3 attempts). \\
    Memory and provenance & The state object accumulates intermediate results and provenance information (such as source URLs, page numbers, or bounding boxes), supporting traceability and robust downstream processing. \\
    Dynamic routing & The graph structure allows for conditional branching and dynamic rerouting based on agent outputs or validation results, ensuring resilient and adaptive execution. \\
    \hline
  \end{tabular}
\end{table}

This approach enables modular, interpretable, and memory-aware multi-agent workflows, with robust error handling and efficient state management throughout the pipeline.

\section{Source Ranking Heuristic}
When ranking candidate PDF URLs for datasheet retrieval, the system evaluates each source using a composite ranking score that reflects both technical and content-based criteria. For each candidate URL $u$, the following factors are considered:
\begin{itemize}
    \item \textbf{Domain Trust Level:} Manufacturer domains are preferred (score 0), followed by trusted distributors (score 1), with all other domains scored as 2.
    \item \textbf{Direct PDF Link:} URLs ending with \texttt{.pdf} are favored (score 0), while indirect or non-PDF links are penalized (score 1).
    \item \textbf{Historical Failure Rate:} An exponentially decayed average of previous download failures from the same host, to deprioritize unreliable sources.
    \item \textbf{PDF Size and Number of Pages:} The system first checks if the file size and page count are within acceptable ranges (e.g., 50\,kB--20\,MB and 1--20 pages). If so, the PDF is downloaded and checked individually; otherwise, the candidate is skipped.
    \item \textbf{Keyword Presence:} After downloading, the PDF (or its metadata) is scanned for key terms such as ``specification,'' ``datasheet,'' or the target component identifier, which can boost the ranking score for matches.
\end{itemize}
The overall ranking score is computed as a weighted sum of these features:
\begin{equation}
\label{eq:heuristic}
\resizebox{\linewidth}{!}{$
\mathrm{score}(u) = w_1 \cdot \mathrm{domain}(u) + w_2 \cdot \mathrm{pdfLink}(u) + w_3 \cdot \mathrm{failRate}(u) - w_4 \cdot \mathrm{keywordBonus}(u)
$}
\end{equation}
where $w_1, \ldots, w_4$ are tunable weights reflecting the importance of each criterion. Lower ranking scores indicate higher-priority candidates for download and parsing.

\section{Data Storage and Logging}
After downloading each PDF, the file is uploaded to Azure Blob Storage. The Azure Blob link, together with its original source URL, is stored in a PostgreSQL database hosted on Azure. The validated specification fields are stored directly in dedicated columns of the PostgreSQL table (rather than as a JSON blob), along with the \verb|component_identifier|, provenance information, and the Azure Blob link and origin.

\section{Mapping to Requirements}
Table~\ref{tab:reqmap} summarises how architectural components satisfy the automation requirements defined in Chapter~\ref{chapter:problem}.

\begin{table}[ht]
  \centering
  \caption{Requirements coverage by architectural component.}
  \label{tab:reqmap}
  \begin{tabular}{p{3.7cm} p{8.8cm}}
    \hline
    \textbf{Requirement} & \textbf{Fulfilled by} \\\hline
    R1 Robust component identifier extraction & Coordinator Agent; Component Identifier Extraction Tool (GPT-4o) \\
    R2 Adaptive retrieval & API and PDF Retrieval Agents; heuristic ranking; LangGraph dynamic routing \\
    R3 Multi-modal parsing & PDF Retrieval Agent; VLM Parsing (GPT-4 Vision) \\
    R4 Schema validation & Validation Module (JSON Schema, range rules, retry/fallback logic) \\
    R5 Traceability & Provenance storage (state object: page, bbox, source URL) \\
    R6 Scalability & LangGraph concurrency; async Playwright \\
    \hline
  \end{tabular}
\end{table}

Together, these design choices create a fault-tolerant, interpretable, and extensible architecture capable of meeting stringent data-quality and throughput requirements.
