\chapter{Related Work}\label{chapter:relatedwork}

This chapter surveys prior research relevant to the proposed multi-agent pipeline, grouped into four areas: (i) tool-using language-model agents, (ii) PDF data-extraction techniques, (iii) multi-agent orchestration frameworks, and (iv) applications of procurement with technical parameters of electronic components, with a particular focus on machine learning for price prediction.

%--------------------------------------------------
\section{Tool-Using Language-Model Agents}
Large language models (LLMs) have been extended with \emph{tool use} capabilities to overcome context-length limits and provide up-to-date information.  
ReAct~\cite{yao2023react} interleaves chain-of-thought reasoning with executable actions, while the OpenAI function-calling interface popularised structured tool invocation in production systems.  
LangChain\footnote{\url{https://www.langchain.com}} offers an open-source abstraction layer for wrapping external tools (APIs, SQL, web search) behind LLM calls.  
AutoGPT and Voyager~\cite{wang2023voyager} push autonomy further but lack guarantees on reliability—highlighting the need for validation mechanisms such as those proposed in this thesis.

\subsection{Origins of Tool Use in LLMs}
Early efforts to augment language models with external tools arose from the need to overcome the inherent limitations of static model knowledge and fixed context windows. Initial approaches included calculator plugins, retrieval-augmented generation (RAG) \cite{lewis_retrieval-augmented_2021}, and code execution modules, allowing LLMs to access up-to-date information, perform precise computations, or fetch facts beyond their training data. These developments laid the groundwork for more sophisticated agentic behaviours by demonstrating that LLMs could be made more useful and reliable when paired with external resources.

\subsection{ReAct-style Reasoning + Acting}
The ReAct framework~\cite{yao2023react} introduced a paradigm in which language models interleave chain-of-thought reasoning with tool invocation, enabling iterative planning, acting, and reflection. In this setup, the model generates both natural language rationales and explicit action commands (e.g., search, lookup, calculate), then observes the results and continues reasoning. This approach improves transparency and sample efficiency, and has inspired a range of agentic systems that combine LLM reasoning with external tool use for complex, multi-step tasks.

\subsection{Structured Function-Calling Interfaces}
The introduction of structured function-calling APIs—most notably by OpenAI—enabled LLMs to invoke tools via well-defined JSON schemas rather than free-form text. This shift improved reliability, composability, and safety in production systems, as models could now output arguments for specific functions (e.g., web search, database query, calculator) in a machine-readable format. Frameworks such as LangChain further abstracted tool integration, making it easier to wrap APIs, SQL engines, or web search behind LLM calls and orchestrate multi-step workflows.

\subsection{Autonomous \& Memory-Augmented Agents}
Recent advances such as AutoGPT and Voyager~\cite{wang2023voyager} have pushed toward greater autonomy by equipping LLM agents with persistent memory, self-reflection, and the ability to pursue open-ended goals over multiple steps. These agents can decompose tasks, store intermediate results, and adapt their strategies based on feedback. However, they often lack robust validation or reliability guarantees, which can lead to error propagation or hallucinated actions—highlighting the need for the validation and provenance mechanisms proposed in this thesis.

%--------------------------------------------------
\section{PDF Data-Extraction Techniques}
Document–analysis research has evolved through five overlapping paradigms:  
(1) \emph{text-only parsing libraries},  
(2) \emph{OCR + classic computer vision},  
(3) \emph{template/rule-based enterprise systems},  
(4) \emph{layout-aware pre-trained transformers}, and  
(5) \emph{large vision--language models}.  
We review each paradigm and highlight the limitations that motivate our approach.

\subsection{Text-Only Parsing Libraries}
Early pipelines extracted text and tables using Python libraries such as
\texttt{PDFMiner}, \texttt{PyPDF2}, \texttt{pdfplumber}, and
\texttt{Camelot}/\texttt{Tabula}.%
\footnote{See project pages: \url{https://github.com/pdfminer/pdfminer.six},
\url{https://github.com/py-pdf/pypdf}, etc.}  
Domain practitioners layered regular expressions, heuristic column detection,
and X-Y–coordinate clustering on these low-level primitives.
While effective for homogeneous reports or invoices, such systems break on

\begin{enumerate}
  \item multi-column datasheets with mixed units,
  \item vendor-specific typographic quirks (rotated headers, unusual glyphs),
  \item text embedded inside nested tables or header/footer bands, and
  \item heterogeneous page layouts within a single file.
\end{enumerate}

They also provide no learned representation of visual context, forcing every
format change to be patched manually.

\subsection{OCR + Classic Computer Vision}
Document images or scanned PDFs that lack embedded text require \emph{image–based} pipelines. Pages are first rasterised and passed through optical character recognition (OCR) engines such as \texttt{Tesseract} or \texttt{EasyOCR}. Classical computer-vision heuristics—connected-component analysis, Hough line transforms, and morphological operations—then group words into lines, columns, and tables. Hybrid toolkits like LayoutParser~\cite{shen2021layoutparser} wrap Detectron2 detectors for layout elements and expose Python APIs for rule-based post-processing. While OCR-centric approaches successfully recover text from low-quality scans, they remain sensitive to resolution, skew, and noise, and provide limited semantic structure beyond bounding boxes.

\subsection{Template/Rule-Based Enterprise Systems}
Commercial "intelligent document processing" platforms—such as ABBYY FlexiCapture, Kofax Transformation, and Google Document AI's \emph{Procure-to-Pay} solution—let practitioners define extraction templates by drawing zones or specifying regular expressions. Rule-based engines deliver high precision for repetitive forms (invoices, tax statements) and support human-in-the-loop correction. However, every new supplier layout or minor format change demands manual template maintenance, creating scalability bottlenecks in heterogeneous procurement workflows.

\subsection{Layout-Aware Pre-Trained Transformers}
Transformer encoders that ingest token embeddings augmented with 2-D
coordinates alleviate fragile heuristics.
LayoutLMv3~\cite{xuhuang2022layoutlmv3} unifies text, layout, and image
patches; StrucTexT~\cite{li2021structext} adds contrastive pre-training for
table structure.
These models excel on benchmarks such as FUNSD and RVL-CDIP, yet published
studies seldom target engineering datasheets whose key attributes are numeric
ranges buried in dense specification tables.

\subsection{Large Vision--Language Models}
General-purpose Llama3, Gemini, and GPT-4 Vision—combine a frozen vision
encoder with a large language model, enabling open-vocabulary reasoning across
arbitrary documents.
Although early demonstrations highlight science articles and receipts,
rigorous evaluation on \emph{technical component datasheets} is scarce.
Our work fills this gap: a GPT-4 Vision–based \textbf{Parsing Agent} is
prompt-engineered to output \emph{schema-validated JSON} containing technical parameters.
This focus on reliability contrasts with prior VLM
applications that prioritise free-form captioning or QA.

%--------------------------------------------------
\section{Multi-Agent Orchestration Frameworks}
Multi-agent orchestration frameworks enable the decomposition of complex tasks into specialised roles, each handled by a dedicated agent or LLM instance. This paradigm draws inspiration from human teams, where members assume distinct responsibilities and collaborate to achieve a shared objective. Recent advances have produced a variety of frameworks that facilitate agent communication, tool integration, and workflow management.

\paragraph{MetaGPT}~\cite{hong2023metagpt} formalises the assignment of roles such as product manager, architect, and engineer to separate LLM agents, each with tailored prompts and objectives. Agents interact via structured messages, simulating a software development team. MetaGPT demonstrates improved task decomposition and code generation quality in software engineering scenarios.

\paragraph{CrewAI}\footnote{\url{https://github.com/joaomdmoura/crewAI}} provides a lightweight Python framework for defining agent "crews" with explicit roles, memory, and tool access. CrewAI supports both synchronous and asynchronous agent execution, and allows for the orchestration of complex workflows such as document analysis, data extraction, and summarisation. Its modular design enables easy integration with external APIs and custom tools.

\paragraph{OpenAI Assistants API} offers a managed environment for creating and coordinating multiple LLM-powered assistants, each with its own instructions, tools, and retrieval capabilities. The API abstracts away low-level orchestration details, making it accessible for rapid prototyping of multi-agent applications.

Despite these advances, most published studies emphasise qualitative demonstrations—such as collaborative story writing or code review—rather than rigorous, quantitative evaluation on real-world, noisy industrial tasks. In particular, there is a lack of empirical benchmarks for multi-agent systems operating on semi-structured documents like quotation analysis templates, where robustness and accuracy are critical.

Our work addresses this gap by providing quantitative results on multi-agent orchestration in a production-grade setting, specifically targeting the extraction and validation of technical parameters from supplier documents. We evaluate not only the correctness of individual agents, but also the reliability and efficiency of the overall workflow under realistic conditions.

\subsection{AutoGen}
AutoGen~\cite{wu2023autogen} is a generic multi-agent conversation framework that enables the creation of complex agent workflows through programmable agent definitions and message passing. Agents can be LLMs, humans, or tools, and can be composed into arbitrary topologies (e.g., chains, trees, or graphs). AutoGen supports advanced features such as self-reflection, tool invocation, and dynamic role assignment. It has been used for tasks ranging from code generation to data analysis, but published evaluations focus primarily on synthetic or well-structured tasks.

\subsection{CrewAI}
CrewAI, as described above, is designed for rapid prototyping of multi-agent systems. It allows developers to define agent roles, assign tools, and specify communication protocols. CrewAI's memory and context management features enable agents to maintain state across multiple interactions, which is essential for tasks involving long documents or multi-step reasoning. While CrewAI has been demonstrated on document summarisation and extraction tasks, systematic evaluation on industrial-scale, noisy data remains limited.

\subsection{LangGraph}
LangGraph\footnote{\url{https://github.com/langchain-ai/langgraph}} is an open-source framework built on top of LangChain, specialising in graph-based orchestration of LLM agents and tools. LangGraph enables the construction of directed acyclic graphs (DAGs) where each node represents an agent or processing step, and edges define data flow and execution order. This approach facilitates modular, fault-tolerant pipelines that can be easily extended or reconfigured. LangGraph supports features such as stateful execution, error handling, and parallelism, making it well-suited for production environments. In our work, LangGraph is used to coordinate agents responsible for MPN extraction, datasheet retrieval, parameter parsing, and validation, ensuring robust and traceable processing of complex supplier documents.

%--------------------------------------------------
\section{Machine Learning for Price Prediction in Procurement}
A key motivation for extracting structured technical data from supplier documents is to enable downstream applications such as automated price prediction. Machine learning models, particularly for tabular data, have been widely adopted in procurement and cost estimation to predict the unit price of electronic components and other industrial parts. These models leverage features such as voltage, current, power, temperature, package type, and supplier region—attributes that are often buried in unstructured datasheets or quotation templates.

Traditional approaches relied on rule-based or statistical models, but recent work has demonstrated the effectiveness of supervised learning methods, including linear regression, gradient-boosted decision trees (e.g., XGBoost), and deep tabular models (e.g., TabNet), for price prediction tasks. The accuracy of these models depends critically on the quality and completeness of the input features, underscoring the importance of robust data extraction pipelines.

By integrating multi-agent document understanding with machine learning–based cost modeling, it becomes possible to automate early-stage price estimation, support supplier negotiations, and drive data-driven procurement decisions. Our work builds on this literature by demonstrating an end-to-end pipeline that not only extracts technical parameters with high fidelity, but also feeds them into predictive models to quantify business value.

\subsection{Classical Parametric and Statistical Models}
Early cost–estimation research in manufacturing employed \emph{parametric} models that express unit price as a linear or polynomial function of explanatory variables such as material type, weight, and production volume. Multiple linear regression and analysis-of-variance (ANOVA) techniques remain popular in industry standards such as \emph{Should-Cost} analysis~\cite{ellram2013shouldcost}. Although interpretable, these models struggle with non-linear interactions and sparse categorical variables common in electronics procurement (e.g., package type × temperature rating).

\subsection{Tree-Based Gradient Boosting}
Ensemble methods such as XGBoost~\cite{chen2016xgboost}, LightGBM, and CatBoost dominate machine-learning competitions on tabular data thanks to their ability to capture non-linear feature interactions, handle missing values, and provide calibrated feature-importance scores. Li \emph{et al.}~\cite{li2020pcbe} showed that gradient-boosted trees predict printed-circuit-board (PCB) costs within 8\% mean absolute percentage error (MAPE), outperforming linear baselines by 35\%. Recent procurement case studies~\cite{lee2022procurementgbdt} report similar gains for semiconductor components.

\subsection{Deep Neural Networks for Tabular Data}
While transformers revolutionised NLP and CV, their success on purely tabular datasets is more recent. Models such as TabNet~\cite{arik2021tabnet}, FT-Transformer~\cite{gorishniy2021revisiting}, and SAINT~\cite{somepalli2021saint} employ attention mechanisms or sequential feature masking to learn hierarchical interactions without handcrafted features. On Huawei's \emph{BOM-Price} dataset, FT-Transformer reduces MAPE from 11.2\% (LightGBM) to 9.6\%~\cite{wang2023bomprice}. However, deep models require larger datasets and careful regularisation to avoid overfitting in high-cardinality supplier features.

\subsection{Hybrid and Multi-Modal Approaches}
Industrial cost prediction often benefits from combining structured BOM attributes with unstructured signals (datasheets, supplier news, macro-economic indices). Huang \emph{et al.}~\cite{huang2022multimodal} fuse numeric features with BERT embeddings of part descriptions, achieving a 12\% relative error reduction over tabular-only baselines. Joint modelling underscores the value of reliable text-to-structure extraction—the focus of our upstream agents.

\subsection{Challenges and Open Problems}
Despite progress, three challenges persist: (i) \textbf{data sparsity}—new MPNs appear faster than historical prices can be collected; (ii) \textbf{distribution shift} caused by supply-chain shocks (e.g., silicon shortages); and (iii) \textbf{feature incompleteness} when key electrical parameters are missing. Active-learning frameworks~\cite{settles2012active} and domain-adaptation techniques~\cite{wilson2020survey} have been proposed but lack validation on procurement benchmarks. Our pipeline tackles feature incompleteness by tightly coupling extraction confidence with model inference, enabling conditional retraining when uncertainty exceeds a cost-of-error threshold.

%--------------------------------------------------
\section{Research Gap}
\begin{itemize}
  \item There is a lack of research evaluating the extraction of electronic Manufacturer Part Numbers (MPNs) from noisy procurement templates, despite the prevalence of such documents in industrial supply chains.
  \item Vision-language models (VLMs) are rarely assessed on the extraction of fine-grained numeric specifications from technical PDFs, with most studies focusing on captioning or general document understanding.
  \item No prior work jointly optimises both the extraction of technical parameters and the quantification of downstream business value (e.g., cost prediction) in a unified, end-to-end industrial pipeline.
\end{itemize}
Addressing these gaps, the following chapters propose and evaluate a multi-agent system designed for robust MPN extraction, technical parameter parsing, and business impact assessment in real-world procurement scenarios.
