\chapter{Related Work}\label{chapter:relatedwork}

This chapter surveys prior research relevant to the proposed multi-agent pipeline, grouped into four areas: (i) tool-using language-model agents, (ii) PDF data-extraction techniques, (iii) multi-agent orchestration frameworks, and (iv) applications of procurement with technical parameters of electronic components, with a particular focus on machine learning for price prediction.

%--------------------------------------------------
\section{Tool-Using Language-Model Agents}
Large language models (LLMs) have been extended with \emph{tool use} capabilities to overcome context-length limits and provide up-to-date information.  
ReAct~\cite{yao2023react} interleaves chain-of-thought reasoning with executable actions, while the OpenAI function-calling interface popularised structured tool invocation in production systems.  
LangChain\footnote{\url{https://www.langchain.com}} offers an open-source abstraction layer for wrapping external tools (APIs, SQL, web search) behind LLM calls.  
AutoGPT and Voyager~\cite{wang2023voyager} push autonomy further but lack guarantees on reliability—highlighting the need for validation mechanisms such as those proposed in this thesis.

\subsection{Origins of Tool Use in LLMs}
Early efforts to augment language models with external tools arose from the need to overcome the inherent limitations of static model knowledge and fixed context windows. Initial approaches included calculator plugins, retrieval-augmented generation (RAG) \cite{lewis_retrieval-augmented_2021}, and code execution modules, allowing LLMs to access up-to-date information, perform precise computations, or fetch facts beyond their training data. These developments laid the groundwork for more sophisticated agentic behaviours by demonstrating that LLMs could be made more useful and reliable when paired with external resources.

\subsection{ReAct-style Reasoning + Acting}
The ReAct framework~\cite{yao2023react} introduced a paradigm in which language models interleave chain-of-thought reasoning with tool invocation, enabling iterative planning, acting, and reflection. In this setup, the model generates both natural language rationales and explicit action commands (e.g., search, lookup, calculate), then observes the results and continues reasoning. This approach improves transparency and sample efficiency, and has inspired a range of agentic systems that combine LLM reasoning with external tool use for complex, multi-step tasks.

\subsection{Structured Function-Calling Interfaces}
The introduction of structured function-calling APIs—most notably by OpenAI—enabled LLMs to invoke tools via well-defined JSON schemas rather than free-form text. This shift improved reliability, composability, and safety in production systems, as models could now output arguments for specific functions (e.g., web search, database query, calculator) in a machine-readable format. Frameworks such as LangChain further abstracted tool integration, making it easier to wrap APIs, SQL engines, or web search behind LLM calls and orchestrate multi-step workflows.

\subsection{Autonomous \& Memory-Augmented Agents}
Recent advances such as AutoGPT and Voyager~\cite{wang2023voyager} have pushed toward greater autonomy by equipping LLM agents with persistent memory, self-reflection, and the ability to pursue open-ended goals over multiple steps. These agents can decompose tasks, store intermediate results, and adapt their strategies based on feedback. However, they often lack robust validation or reliability guarantees, which can lead to error propagation or hallucinated actions—highlighting the need for the validation and provenance mechanisms proposed in this thesis.

%--------------------------------------------------
\section{PDF Data-Extraction Techniques}
Document–analysis research has evolved through five overlapping paradigms:  
(1) \emph{text-only parsing libraries},  
(2) \emph{OCR + classic computer vision},  
(3) \emph{template/rule-based enterprise systems},  
(4) \emph{layout-aware pre-trained transformers}, and  
(5) \emph{large vision--language models}.  
We review each paradigm and highlight the limitations that motivate our approach.

\subsection{Text-Only Parsing Libraries}
Early pipelines extracted text and tables using Python libraries such as
\texttt{PDFMiner}, \texttt{PyPDF2}, \texttt{pdfplumber}, and
\texttt{Camelot}/\texttt{Tabula}.%
\footnote{See project pages: \url{https://github.com/pdfminer/pdfminer.six},
\url{https://github.com/py-pdf/pypdf}, etc.}  
Domain practitioners layered regular expressions, heuristic column detection,
and X-Y–coordinate clustering on these low-level primitives.
While effective for homogeneous reports or invoices, such systems break on

\begin{enumerate}
  \item multi-column datasheets with mixed units,
  \item vendor-specific typographic quirks (rotated headers, unusual glyphs),
  \item text embedded inside nested tables or header/footer bands, and
  \item heterogeneous page layouts within a single file.
\end{enumerate}

They also provide no learned representation of visual context, forcing every
format change to be patched manually.

\subsection{OCR + Classic Computer Vision}
Document images or scanned PDFs that lack embedded text require \emph{image–based} pipelines. Pages are first rasterised and passed through optical character recognition (OCR) engines such as \texttt{Tesseract} or \texttt{EasyOCR}. Classical computer-vision heuristics—connected-component analysis, Hough line transforms, and morphological operations—then group words into lines, columns, and tables. Hybrid toolkits like LayoutParser~\cite{shen2021layoutparser} wrap Detectron2 detectors for layout elements and expose Python APIs for rule-based post-processing. While OCR-centric approaches successfully recover text from low-quality scans, they remain sensitive to resolution, skew, and noise, and provide limited semantic structure beyond bounding boxes.

\subsection{Template/Rule-Based Enterprise Systems}
Commercial "intelligent document processing" platforms—such as ABBYY FlexiCapture, Kofax Transformation, and Google Document AI's \emph{Procure-to-Pay} solution—let practitioners define extraction templates by drawing zones or specifying regular expressions. Rule-based engines deliver high precision for repetitive forms (invoices, tax statements) and support human-in-the-loop correction. However, every new supplier layout or minor format change demands manual template maintenance, creating scalability bottlenecks in heterogeneous procurement workflows.

\subsection{Layout-Aware Pre-Trained Transformers}
Transformer encoders that ingest token embeddings augmented with 2-D
coordinates alleviate fragile heuristics.
LayoutLMv3~\cite{xuhuang2022layoutlmv3} unifies text, layout, and image
patches; StrucTexT~\cite{li2021structext} adds contrastive pre-training for
table structure.
These models excel on benchmarks such as FUNSD and RVL-CDIP, yet published
studies seldom target engineering datasheets whose key attributes are numeric
ranges buried in dense specification tables.

\subsection{Large Vision--Language Models}
General-purpose Llama3, Gemini, and GPT-4 Vision—combine a frozen vision
encoder with a large language model, enabling open-vocabulary reasoning across
arbitrary documents.
Although early demonstrations highlight science articles and receipts,
rigorous evaluation on \emph{technical component datasheets} is scarce.
Our work fills this gap: a GPT-4 Vision–based \textbf{Parsing Agent} is
prompt-engineered to output \emph{schema-validated JSON} containing technical parameters.
This focus on reliability contrasts with prior VLM
applications that prioritise free-form captioning or QA.

%--------------------------------------------------
\section{Multi-Agent Orchestration Frameworks}
Multi-agent orchestration frameworks enable the decomposition of complex tasks into specialised roles, each handled by a dedicated agent or LLM instance. This paradigm draws inspiration from human teams, where members assume distinct responsibilities and collaborate to achieve a shared objective. Recent advances have produced a variety of frameworks that facilitate agent communication, tool integration, and workflow management.

\paragraph{MetaGPT}~\cite{hong2023metagpt} formalises the assignment of roles such as product manager, architect, and engineer to separate LLM agents, each with tailored prompts and objectives. Agents interact via structured messages, simulating a software development team. MetaGPT demonstrates improved task decomposition and code generation quality in software engineering scenarios.

\paragraph{CrewAI}\footnote{\url{https://github.com/joaomdmoura/crewAI}} provides a lightweight Python framework for defining agent "crews" with explicit roles, memory, and tool access. CrewAI supports both synchronous and asynchronous agent execution, and allows for the orchestration of complex workflows such as document analysis, data extraction, and summarisation. Its modular design enables easy integration with external APIs and custom tools.

\paragraph{OpenAI Assistants API} offers a managed environment for creating and coordinating multiple LLM-powered assistants, each with its own instructions, tools, and retrieval capabilities. The API abstracts away low-level orchestration details, making it accessible for rapid prototyping of multi-agent applications.

Despite these advances, most published studies emphasise qualitative demonstrations—such as collaborative story writing or code review—rather than rigorous, quantitative evaluation on real-world, noisy industrial tasks. In particular, there is a lack of empirical benchmarks for multi-agent systems operating on semi-structured documents like quotation analysis templates, where robustness and accuracy are critical.

Our work addresses this gap by providing quantitative results on multi-agent orchestration in a production-grade setting, specifically targeting the extraction and validation of technical parameters from supplier documents. We evaluate not only the correctness of individual agents, but also the reliability and efficiency of the overall workflow under realistic conditions.

\subsection{AutoGen}
AutoGen~\cite{wu2023autogen} is a generic multi-agent conversation framework that enables the creation of complex agent workflows through programmable agent definitions and message passing. Agents can be LLMs, humans, or tools, and can be composed into arbitrary topologies (e.g., chains, trees, or graphs). AutoGen supports advanced features such as self-reflection, tool invocation, and dynamic role assignment. It has been used for tasks ranging from code generation to data analysis, but published evaluations focus primarily on synthetic or well-structured tasks.

\subsection{CrewAI}
CrewAI, as described above, is designed for rapid prototyping of multi-agent systems. It allows developers to define agent roles, assign tools, and specify communication protocols. CrewAI's memory and context management features enable agents to maintain state across multiple interactions, which is essential for tasks involving long documents or multi-step reasoning. While CrewAI has been demonstrated on document summarisation and extraction tasks, systematic evaluation on industrial-scale, noisy data remains limited.

\subsection{LangGraph}
LangGraph\footnote{\url{https://github.com/langchain-ai/langgraph}} is an open-source framework built on top of LangChain, specialising in graph-based orchestration of LLM agents and tools. LangGraph enables the construction of directed acyclic graphs (DAGs) where each node represents an agent or processing step, and edges define data flow and execution order. This approach facilitates modular, fault-tolerant pipelines that can be easily extended or reconfigured. LangGraph supports features such as stateful execution, error handling, and parallelism, making it well-suited for production environments. In our work, LangGraph is used to coordinate agents responsible for MPN extraction, datasheet retrieval, parameter parsing, and validation, ensuring robust and traceable processing of complex supplier documents.

%--------------------------------------------------
\section{Machine Learning for Price Prediction}
Machine learning models for tabular data have become a standard approach for predicting prices and costs in a variety of domains, including manufacturing, retail, and supply chain management. These models leverage structured features such as technical specifications, categorical attributes, and contextual information to estimate target values like unit price or production cost. The effectiveness of these models depends on the quality and completeness of the input features, which are often extracted from unstructured or semi-structured sources.

Traditional approaches to price prediction relied on rule-based systems or simple statistical models. More recently, supervised learning methods—including linear regression, gradient-boosted decision trees, and deep neural networks—have demonstrated superior performance on a range of price prediction tasks. The choice of model and feature engineering strategy is often dictated by the complexity of the data and the presence of non-linear interactions.

\subsection{Classical Parametric and Statistical Models}
Early work in price and cost estimation often used \emph{parametric} models, expressing the target variable as a linear or polynomial function of explanatory features such as material type, weight, or production volume. Multiple linear regression and analysis-of-variance (ANOVA) remain popular due to their interpretability and ease of use. However, these models can struggle with non-linear relationships and high-cardinality categorical variables.

\subsection{Tree-Based Gradient Boosting}
Ensemble methods such as XGBoost~\cite{chen2016xgboost}, LightGBM, and CatBoost have become dominant in machine learning competitions involving tabular data. These models are valued for their ability to capture complex feature interactions, handle missing values, and provide interpretable feature-importance scores. Empirical studies have shown that gradient-boosted trees can significantly outperform linear models in price prediction and related regression tasks.

\subsection{Deep Neural Networks for Tabular Data}
While deep learning has revolutionised fields like natural language processing and computer vision, its application to tabular data is more recent. Models such as TabNet~\cite{arik2021tabnet}, FT-Transformer~\cite{gorishniy2021revisiting}, and SAINT~\cite{somepalli2021saint} use attention mechanisms and feature masking to learn complex, hierarchical interactions without extensive manual feature engineering. On challenging datasets, deep tabular models have achieved lower error rates than traditional methods, though they often require larger datasets and careful regularisation to avoid overfitting.

% \subsection{Hybrid and Multi-Modal Approaches}
% Recent research has explored combining structured tabular features with unstructured data sources, such as text descriptions or external signals. For example, Huang \emph{et al.}~\cite{huang2022multimodal} fused numeric features with BERT embeddings of product descriptions, achieving improved accuracy over models using only tabular data. These hybrid approaches highlight the importance of reliable data extraction and representation for downstream predictive modeling.

% \subsection{Challenges and Open Problems}
% Despite advances, several challenges remain: (i) \textbf{data sparsity}, where new products or items appear faster than labeled data can be collected; (ii) \textbf{distribution shift}, due to changes in market conditions or external shocks; and (iii) \textbf{feature incompleteness}, when key attributes are missing or uncertain. Active learning~\cite{settles2012active} and domain adaptation~\cite{wilson2020survey} have been proposed to address these issues, but further validation on real-world benchmarks is needed. Integrating robust data extraction with predictive modeling remains an open area of research.

%--------------------------------------------------
\section{Research Gap}
\begin{itemize}
  \item There is limited research on extracting fine-grained technical attributes from noisy, semi-structured documents for use in downstream machine learning models.
  \item Vision-language models (VLMs) are rarely evaluated on the extraction of detailed numeric specifications from technical PDFs, with most studies focusing on tasks like captioning or general document understanding.
  \item Few works jointly optimise both the extraction of technical parameters and the quantification of downstream predictive performance in a unified, end-to-end pipeline.
\end{itemize}
Addressing these gaps, the following chapters propose and evaluate a multi-agent system designed for robust technical attribute extraction, parameter parsing, and integration with predictive modeling in real-world scenarios.
