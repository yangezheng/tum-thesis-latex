\section{Cost Model}
\label{chapter:costmodel}

%--------------------------------------------------
\subsection{Motivation}
While the previous chapter demonstrates high-fidelity extraction of technical parameters, stakeholders ultimately care about the economic impact: can the structured data reduce effort in early price negotiations?  To quantify this value, we train a machine–learning model that predicts the expected unit price of an electronic component based solely on the pipeline's output.

%--------------------------------------------------
\subsection{Dataset}
We constructed separate datasets for each cost-prediction model, as the relevant technical parameters differ by component type. Each dataset contains distinct part numbers purchased between 2020 and~2023. For every part, we store:
\begin{itemize}
  \item validated specification JSON produced by the pipeline (Section~\ref{chapter:evaluation}), with features tailored to the target model (e.g., voltage/current for power ICs, or capacitance for passives),
  \item historical purchase price (EUR) normalised to a quantity of~1\,000 units, and
  \item supplier region (\textit{Asia}, \textit{Europe}, \textit{Americas}).
\end{itemize}
After removing entries with missing price or incomplete specs, each dataset contains between \textbf{3\,900} and \textbf{4\,400} records, depending on the model. We allocate 70\,\% to training, 15\,\% to validation, and 15\,\% to a held-out test set. The training process and evaluation protocol are identical for all datasets; only the feature sets differ.

%--------------------------------------------------
\subsection{Feature Engineering}
\subsubsection{Numeric Features}
Voltage, current, power, and temperature range are used as raw features and normalised to zero mean and unit variance.

\subsubsection{Categorical Features}
\begin{itemize}
  \item \textbf{Package}: one-hot encoded (e.g., QFN, TQFP).
  \item \textbf{Supplier Region}: three-way one-hot.
\end{itemize}

\subsubsection{Derived Features}
\begin{itemize}
  \item \textbf{Power Density} = \textit{power}~\slash~(package~area).
  \item \textbf{Temp.~Span}   = $T_{\max}-T_{\min}$.
\end{itemize}
Package area is approximated from JEDEC footprint tables.

%--------------------------------------------------
\subsection{Models Evaluated}
\begin{description}
  \item[\textbf{Baseline}] Mean price of training set (naïve predictor).
  \item[\textbf{Linear}] Ordinary least squares on all features.
  \item[\textbf{XGBoost}] Gradient-boosted decision trees (400 estimators, \texttt{max\_depth}=6).
  \item[\textbf{TabNet}] Tabular deep-learning model with default hyper-parameters.
\end{description}
Hyper-parameters were tuned on the validation split via Optuna with 50 trials.

%--------------------------------------------------
\subsection{Evaluation Metrics}
\begin{itemize}
  \item \textbf{MAE}: Mean Absolute Error in EUR.
  \item \textbf{MAPE}: Mean Absolute Percentage Error.
  \item \textbf{$R^{2}$}: Coefficient of determination.
\end{itemize}

%--------------------------------------------------
\subsection{Results}
\begin{table}[H]
\centering
\caption{Price-prediction performance on the held-out test set.}
\label{tab:cost}
\begin{tabular}{lccc}
\toprule
Model & MAE~(EUR) & MAPE~(\%) & $R^{2}$ \\
\midrule
Baseline  & 0.142 & 35.7 & 0.00 \\
Linear    & 0.096 & 24.3 & 0.42 \\
TabNet    & 0.074 & 18.1 & 0.63 \\
XGBoost   & \textbf{0.061} & \textbf{14.9} & \textbf{0.71} \\
\bottomrule
\end{tabular}
\end{table}
XGBoost reduces absolute error by 57\,\% relative to the naïve baseline, highlighting the predictive value of the extracted specifications.

%--------------------------------------------------
\subsection{Feature Importance}
Figure~\ref{fig:shap} shows SHAP values for the XGBoost model.  \textit{Voltage} and \textit{package} contribute most to price variance, followed by \textit{supplier region}.  Power density plays a secondary role, suggesting thermal constraints factor into cost but less strongly than package size.

%--------------------------------------------------
\subsection{Ablation Study}
Removing individual feature groups confirms their additive benefit (Table~\ref{tab:ablation-cost}).

\begin{table}[H]
\centering
\caption{Ablation on XGBoost feature groups.}
\label{tab:ablation-cost}
\begin{tabular}{lcc}
\toprule
Feature set & MAE & $\Delta$MAE \\
\midrule
All features & 0.061 & --- \\
-- Derived    & 0.068 & +0.007 \\
-- Categorical & 0.072 & +0.011 \\
-- Numeric     & 0.109 & +0.048 \\
\bottomrule
\end{tabular}
\end{table}

Numeric electrical parameters are indispensable; categorical and derived features provide incremental gains.

%--------------------------------------------------
\subsection{Discussion}
Despite promising accuracy, two limitations warrant caution: (i) historic prices incorporate volume discounts and contractual nuances not captured by technical specs, and (ii) the dataset under-represents high-power devices, where extrinsic factors (e.g., heatsink requirements) dominate cost.

%--------------------------------------------------
\subsection{Summary}
Leveraging the structured output of the extraction pipeline, an XGBoost regressor achieves sub-0.07~EUR MAE on unseen parts, far outperforming linear and deep-learning baselines.  This demonstrates that high-quality parameter extraction not only aids engineering analysis but also provides actionable pricing insights for procurement teams.