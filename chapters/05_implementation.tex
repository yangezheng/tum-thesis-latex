\chapter{Implementation}
\label{chapter:implementation}

%--------------------------------------------------
\section{Technology Stack}
The system is implemented in \textbf{Python~3.11} and leverages a concise set of robust open-source libraries and cloud services:
\begin{itemize}
  \item \textbf{LangGraph}~\footnote{v0.0.30 at time of writing} for graph-based agent orchestration and workflow management.
  \item \textbf{PyPDF2} and \textbf{pdfminer.six} for lightweight PDF metadata inspection.
  \item \textbf{Azure OpenAI Python SDK} for GPT-4 and GPT-4~Vision function calls.
  \item \textbf{Playwright} to drive a headless Chromium browser for Google search and captcha-free PDF downloads.
  \item \textbf{Azure PostgreSQL~15} for durable storage of validated specification records and provenance.
  \item \textbf{scikit-learn} for training and evaluating both linear regression and random forest cost models.
\end{itemize}

%--------------------------------------------------
\section{Repository Layout}
Listing~\ref{lst:tree} shows the top-level directory structure.  Each agent and utility lives in its own module with clear dependency boundaries.
\begin{figure}[H]
\centering
\begin{minipage}{0.9\textwidth}
\begin{verbatim}
multi_agent_data_enrichment/
  cli.py
  datasheet_download.log
  datasheets/
    LM324.pdf
    LM358.pdf
    LM456.pdf
  debug_screenshots/
  logs/
  pyproject.toml
  README.md
  src/
    multi_agent_data_enrichment/
      __init__.py
      graph/
        component_graph.py
      tools/
        component_tools.py
        datasheet_parse.py
        downloader.py
      visualization.py
  token_usage_log.csv
  uv.lock
\end{verbatim}
\end{minipage}
\caption{Abbreviated repository tree of the multi\_agent\_data\_enrichment project.}
\label{lst:tree}
\end{figure}

%--------------------------------------------------
\section{Agent Implementations}
This section details the concrete logic, prompts, and error-handling behaviour of each agent introduced in Chapter~\ref{chapter:architecture}.

\subsection{Planner Agent}

The Planner Agent serves as the entry point and orchestrator of the workflow. For each input line—whether a raw component description or a direct MPN—it first invokes the MPN Extraction Tool to standardise the input into a canonical MPN and associated categories. Once the MPN is extracted, the Planner Agent determines the appropriate downstream path, forwarding the result to either the API Retrieval Agent or the Web-Search Agent depending on the component type and data availability. This modular approach ensures that all inputs are processed consistently, enabling robust error handling and flexible routing throughout the pipeline.

By always normalising inputs through the extraction tool, the system can seamlessly handle both free-text descriptions and explicit MPNs, streamlining the overall workflow.

\subsection{MPN Extraction Tool}

The MPN Extraction Tool is implemented as a LangChain tool, rather than a standalone agent. It wraps a language model call that takes a component description string and returns the manufacturer part number (MPN) and relevant categories in JSON format. The tool can be invoked by agents or planners within the workflow whenever standardisation or extraction of MPNs is required.

\begin{figure}[H]
\centering
\begin{minipage}{0.9\textwidth}
\begin{verbatim}
Input: "CAP 47nF 10% 50V 0603 X7R Purchased Part GCM188R71H473KA55D MURATA"
Output: {"category_level_1": "passive-components", "category_level_2": "capacitors", "category_level_3": "ceramic-capacitors", "mpn": "GCM188R71H473KA55D"}
\end{verbatim}
\end{minipage}
\caption{Example input and output for the MPN Extraction Tool, which is registered as a LangChain tool.}
\label{lst:mpn_schema}
\end{figure}

\paragraph{Implementation Example: Extraction Tool}
\begin{lstlisting}[language=Python]
from langchain.tools import tool

@tool
def extract_mpn(description: str) -> dict:
    """
    Extracts the MPN and categories from a component description string.
    """
    # Call your language model here with the prompt and parse the JSON output
    response = llm(f'Extract MPN and categories as JSON: "{description}"')
    return json.loads(response)
\end{lstlisting}

\subsection{Retrieval Agents}
\paragraph{Octopart API Wrapper}  Queries are rate-limited to ten requests per second via an async semaphore.  Responses are cached in Redis with a 24-hour TTL keyed by MPN.

\paragraph{Web-Search Agent}  Playwright launches a \texttt{chromium} browser in non-headed, proxy-aware mode and executes the search string:\\
\texttt{\{MPN\} datasheet filetype:pdf}.  Candidate URLs are scored by the heuristic in Equation~\ref{eq:heuristic}.  PDFs larger than 20~MB or smaller than 50~kB are discarded early to save tokens in the parsing step.

\subsection{Parsing Agent (GPT-4 Vision)}
Each downloaded PDF is split into page images which are batched into groups of four to amortise API overhead.  The chain-of-thought prompt first asks GPT-4 V to produce a section outline before extracting the five target parameters (voltage, current, power, temperature, package).  The agent returns both the JSON fields and a list of (page, bbox) tuples for provenance.

\subsection{Validation Module}
Validation combines three layers:
\begin{enumerate}
  \item \textbf{JSON-Schema} checks ensure types and units.
  \item \textbf{Physical range} rules (e.g., $0< V_{\max}<1000\,\mathrm{V}$).
  \item \textbf{Cross-source consistency} compares datasheet values with Octopart metadata; discrepancies >5\,\% trigger a retry path to the Web-Search Agent.
\end{enumerate}
If validation succeeds, the record is written to PostgreSQL and an OpenTelemetry span is emitted.

%--------------------------------------------------
\section{Orchestration and State Management}
Agents are registered as LangChain tools and executed by an \texttt{LCEL} (LangChain Expression Language) graph shown in Figure~\ref{fig:lcel}.  Shared state is held in \texttt{RedisJSON} under the key \texttt{standardised-quotation-analysis-template:\textless{}id\textgreater{}}.  Updates are performed with optimistic locking to avoid lost writes when agents run in parallel.

%--------------------------------------------------
\section{Summary}
This chapter translated the high-level architecture into concrete implementation artefacts: Python modules, prompts, deployment scripts, and observability tooling.  The next chapter evaluates how well these design choices satisfy the performance and accuracy requirements defined earlier.